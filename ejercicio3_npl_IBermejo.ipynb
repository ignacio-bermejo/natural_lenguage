{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    },
    "interpreter": {
      "hash": "642ed197f51b4a6f70d3007f365abeb4e11cf6d289847988e1cf6d37c781e715"
    },
    "colab": {
      "name": "prueba_BERT_xxx.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "!pip install --upgrade tensorflow --user\r\n",
        "!pip install transformers\r\n",
        "!pip install torch"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already up-to-date: tensorflow in c:\\users\\ignac\\appdata\\roaming\\python\\python38\\site-packages (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator~=2.6 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio<2.0,>=1.37.0 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (1.39.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (3.15.8)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=3.1.0 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: clang~=5.0 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (5.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: keras~=2.6 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.4.0 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.6 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.24.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.30.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (50.3.1.post20201107)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.25.11)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: transformers in c:\\users\\ignac\\anaconda3\\lib\\site-packages (4.9.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: requests in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from transformers) (2020.10.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from transformers) (5.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from transformers) (4.50.2)\n",
            "Requirement already satisfied: sacremoses in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: torch in c:\\users\\ignac\\anaconda3\\lib\\site-packages (1.9.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\ignac\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBrKmAylOfUd",
        "outputId": "baf8746d-2d89-41f2-b2ed-f493d47ca6ec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redes recurrentes con capas de atención"
      ],
      "metadata": {
        "id": "37pg9a2pOfUi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "from importlib import reload\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow.keras as keras\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Flatten\r\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\r\n",
        "from tensorflow.keras.models import Model, Sequential\r\n",
        "from tensorflow.keras.layers import Convolution1D\r\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ignac\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ignac\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03il6mVUOfUj",
        "outputId": "ee8bdc14-596e-49eb-ac7f-5460b9cc981f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero se leen las frases financieras con el 100% y se añade una columna de sentimiento, tambien preparamos un segundo dataset donde iremos guardando los datos limpios. Tras esto realimos un bucle en el que separamos la frase del sentimiento."
      ],
      "metadata": {
        "id": "UUxAlGJzOfUl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "#Leemos el documento y creamos las columnas texto y sentimiento\r\n",
        "data=pd.read_csv('C:\\\\Users\\\\ignac\\\\OneDrive\\\\Escritorio\\\\BME\\\\Master\\\\CLASES\\\\Modulo 4\\\\Lenguage Natural\\\\Practica\\\\Financial_Phrases\\\\Sentences_AllAgree.txt',sep='\\n ', header=None)\r\n",
        "data.columns = [\"text\"]\r\n",
        "data.dropna(inplace=True)\r\n",
        "data.insert(1,'sentiment',0)\r\n",
        "data.head()\r\n",
        "dataclean=data*1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-dd18d69d196b>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  data=pd.read_csv('C:\\\\Users\\\\ignac\\\\OneDrive\\\\Escritorio\\\\BME\\\\Master\\\\CLASES\\\\Modulo 4\\\\Lenguage Natural\\\\Practica\\\\Financial_Phrases\\\\Sentences_AllAgree.txt',sep='\\n ', header=None)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "gEEBPuVgOfUl",
        "outputId": "c5c994af-c6b6-4c4c-cd2d-48d1a1a793e4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Separamos el texto original en dos columnas, una para el texto y la otra para el sentimiento\r\n",
        "for a in range(0,len(data)):\r\n",
        "  x = re.search('@\\w', data.iloc[a].text)\r\n",
        "  s = x.start()\r\n",
        "  dataclean.iloc[a,0] = data.iloc[a].text[:s]\r\n",
        "  dataclean.iloc[a,1] = data.iloc[a].text[s+1:]"
      ],
      "outputs": [],
      "metadata": {
        "id": "jIfxakVsOfUm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Definimos las herramientas para la limpieza de texto(stop words y lemmatizer), definimos la funcion de limpieza y la aplicamos creando una nueva columna de texto procesado\r\n",
        "stop_words = set(stopwords.words(\"english\")) \r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "\r\n",
        "def clean_text(text):\r\n",
        "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\r\n",
        "    text = text.lower()\r\n",
        "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\r\n",
        "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\r\n",
        "    text = [word for word in text if not word in stop_words]\r\n",
        "    text = \" \".join(text)\r\n",
        "    return text\r\n",
        "\r\n",
        "dataclean['Processed_Text'] = dataclean.text.apply(lambda x: clean_text(x))\r\n",
        "dataclean.head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "P47lKW49OfUn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#observamos la longitud media de cada frase\r\n",
        "dataclean.text.apply(lambda x: len(x.split(\" \"))).mean()"
      ],
      "outputs": [],
      "metadata": {
        "id": "af8OarUOOfUp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Realizamos el codificado del sentimiento en numeros para poder tratarlo posteriormente en el modelo y hacemos la separacion de los datos en train y test\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "le = LabelEncoder()\r\n",
        "dataclean['sentiment'] = le.fit_transform(dataclean['sentiment'])\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataclean['Processed_Text'],\r\n",
        "                                                    dataclean['sentiment'],\r\n",
        "                                                    test_size=0.2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "iaziz8OAOfUq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Identificamos cuantas etiquetas distintas hay\r\n",
        "np.unique(dataclean['sentiment'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "0YPhv5TeOfUr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#definimos los parametros de nuestras redes\r\n",
        "\r\n",
        "EMBED_SIZE = 5\r\n",
        "RNN_CELL_SIZE = 64\r\n",
        "MAX_LEN = 25   # Since our mean length is 22.45\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "JsMJVP36OfUs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Definimos el tokenizado separando por espacios y posteriormente lo aplicamos al xtrain, ademas definimos el max sequence y el max features que utilizaremos posteriormente\r\n",
        "\r\n",
        "tokenizer = Tokenizer(split=\" \") \r\n",
        "tokenizer.fit_on_texts(X_train)\r\n",
        "sequences = tokenizer.texts_to_sequences(X_train)\r\n",
        "x_train = pad_sequences(sequences, maxlen=25,padding=\"pre\")\r\n",
        "word_index = tokenizer.word_index\r\n",
        "MAX_FEATURES = len(word_index)+1\r\n",
        "max_sequence = x_train.shape[1]"
      ],
      "outputs": [],
      "metadata": {
        "id": "J5LoxrFWOfUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Aplicamos el tokenizado tambien al test\r\n",
        "sequences_test = tokenizer.texts_to_sequences(X_test)\r\n",
        "x_test = pad_sequences(sequences_test, maxlen=25,padding=\"pre\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "0xKn_vHQOfUu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Creamos la capa de atencion, en este caso es la de bahdanau\r\n",
        "\r\n",
        "class Attention(tf.keras.Model):\r\n",
        "    def __init__(self, units):\r\n",
        "        super(Attention, self).__init__()\r\n",
        "        self.W1 = tf.keras.layers.Dense(units)\r\n",
        "        self.W2 = tf.keras.layers.Dense(units)\r\n",
        "        self.V = tf.keras.layers.Dense(1)\r\n",
        " \r\n",
        "    def call(self, features, hidden):\r\n",
        "        # hidden shape == (batch_size, hidden size)\r\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\r\n",
        "        # we are doing this to perform addition to calculate the score\r\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\r\n",
        "\r\n",
        "        # score shape == (batch_size, max_length, 1)\r\n",
        "        # we get 1 at the last axis because we are applying score to self.V\r\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\r\n",
        "        score = tf.nn.tanh(\r\n",
        "            self.W1(features) + self.W2(hidden_with_time_axis))\r\n",
        "        \r\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\r\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\r\n",
        "\r\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\r\n",
        "        context_vector = attention_weights * features\r\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\r\n",
        " \r\n",
        "        return context_vector, attention_weights"
      ],
      "outputs": [],
      "metadata": {
        "id": "psQt2x6aOfUu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Definimos la capa de embeding mediante keras funcional\r\n",
        "sequence_input = Input(shape=(MAX_LEN,), dtype=\"int32\")\r\n",
        "embedded_sequences = Embedding(MAX_FEATURES, EMBED_SIZE)(sequence_input) "
      ],
      "outputs": [],
      "metadata": {
        "id": "WtySlNaGOfUw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Una vez crada la capa de embeding se pueden introducir las capas que se deseen, en este caso se introduce una lstm bidireccional y se conecta con el embeding\r\n",
        "lstm = Bidirectional(LSTM(RNN_CELL_SIZE, return_sequences = True), name=\"bi_lstm_0\")(embedded_sequences)\r\n",
        "# Recogiendo los outputs de la LSTM , al ser bidireccional tenemos la informacion forward ademas de la backward\r\n",
        "(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(RNN_CELL_SIZE, return_sequences=True, return_state=True), name=\"bi_lstm_1\")(lstm)"
      ],
      "outputs": [],
      "metadata": {
        "id": "-b-MYFhsOfUw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Dado que nuestro modelo utiliza un RNN bidireccional, primero concatenamos los estados ocultos de cada RNN antes de calcular los pesos de atención y aplicar la suma ponderada.\r\n",
        "#primero concatenamos la informacion hacia delante y hacia detras de las sequences y los states\r\n",
        "state_h = Concatenate()([forward_h, backward_h])\r\n",
        "state_c = Concatenate()([forward_c, backward_c])\r\n",
        "#a la capa de atencion le pasamos las salidas de la lstm y los estados ocultos\r\n",
        "context_vector, attention_weights = Attention(10)(lstm, state_h)\r\n",
        "#con el vector contexto se le introduce a una densa para realizar el problema de clasificacion con una capa dropout\r\n",
        "\r\n",
        "dense2 = Dense(20, activation=\"relu\")(context_vector)\r\n",
        "dropout2 = Dropout(0.5)(dense2)\r\n",
        "#Al tener 3 clasificaciones(pos, neg y neutro) se introduce una capa de salida con 3 neuronas \r\n",
        "output = Dense(3, activation=\"softmax\")(dropout2)\r\n",
        "#Definimos el modelo, sus entradas y salidas.\r\n",
        "model = keras.Model(inputs=sequence_input, outputs=output)"
      ],
      "outputs": [],
      "metadata": {
        "id": "rXTNCFVEOfUx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#vemos un resumen del modelo\r\n",
        "print(model.summary())"
      ],
      "outputs": [],
      "metadata": {
        "id": "BsYL3g3IOfUx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#pintamos el modelo \r\n",
        "!pip install pydot\r\n",
        "#!pip install graphviz\r\n",
        "#!winget install graphviz\r\n",
        "\r\n",
        "keras.utils.plot_model(model, show_shapes=True, dpi=90)"
      ],
      "outputs": [],
      "metadata": {
        "id": "9OLDSjqKOfUy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Compilamos el modelo\r\n",
        "METRICS = [\r\n",
        "      keras.metrics.TruePositives(name='tp'),\r\n",
        "      keras.metrics.FalsePositives(name='fp'),\r\n",
        "      keras.metrics.TrueNegatives(name='tn'),\r\n",
        "      keras.metrics.FalseNegatives(name='fn'), \r\n",
        "      keras.metrics.Accuracy(name='accuracy'),\r\n",
        "      keras.metrics.Precision(name='precision'),\r\n",
        "      keras.metrics.Recall(name='recall'),\r\n",
        "      keras.metrics.AUC(name='auc'),\r\n",
        "]\r\n",
        "\r\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "0DoiC_ZaOfUy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Entrenaremos nuestro modelo de atención por épocas de 5 en mini lotes de muestras de 100.\r\n",
        "BATCH_SIZE = 100\r\n",
        "EPOCHS = 30\r\n",
        "history = model.fit(x_train,y_train,\r\n",
        "                    batch_size=BATCH_SIZE,\r\n",
        "                    epochs=EPOCHS,\r\n",
        "                    validation_split=0.2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "K5Xl9Ef8OfUy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Evaluamos sobre test y sacamos el accuracy del test\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "prediction = model.predict(x_test)\r\n",
        "y_pred = (prediction > 0.5)\r\n",
        "accuracy_score(y_test, np.argmax(y_pred, axis=1))"
      ],
      "outputs": [],
      "metadata": {
        "id": "qzx4TMEfOfUz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Definimos la matriz de confusion y la ploteamos\r\n",
        "import numpy as np\r\n",
        "import matplotlib as mpl\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.metrics import (classification_report,\r\n",
        "                             confusion_matrix,\r\n",
        "                             roc_auc_score)\r\n",
        "%matplotlib inline\r\n",
        "%config InlineBackend.figure_format = 'retina'\r\n",
        "\r\n",
        "\r\n",
        "def plot_cm(labels, predictions, p=0.5):\r\n",
        "    cm = confusion_matrix(y_test, np.argmax(y_pred,axis=1))\r\n",
        "    plt.figure(figsize=(5, 5))\r\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\")\r\n",
        "    plt.title(\"Confusion matrix (non-normalized))\")\r\n",
        "    plt.ylabel(\"Actual label\")\r\n",
        "    plt.xlabel(\"Predicted label\")\r\n",
        "\r\n",
        "\r\n",
        "plot_cm(y_test, y_pred)"
      ],
      "outputs": [],
      "metadata": {
        "id": "2S__l9tROfU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONVOLUCIONAL"
      ],
      "metadata": {
        "id": "c3WVjcX8OfU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout"
      ],
      "outputs": [],
      "metadata": {
        "id": "dRmVaQS6OfU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#con el mismo preprocesado del apartado anterior combino capas recurrentes y convolucionales\r\n",
        "model = tf.keras.Sequential()\r\n",
        "#Definimos la capa de embeding \r\n",
        "model.add(tf.keras.layers.Embedding(MAX_FEATURES,5,input_length=x_train.shape[1]))\r\n",
        "#se introduce una lstm bidireccional y se conecta con el embeding\r\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.3,return_sequences=True)))\r\n",
        "#Añadimos la capa convolucional y capas de regularizacion (en este caso max pooling y dropout) para evitar el sobreentrenamiento\r\n",
        "model.add(Conv1D(10, 2,padding=\"valid\",activation=\"selu\",strides=1))\r\n",
        "model.add(GlobalMaxPooling1D())\r\n",
        "model.add(Dropout(0.7))\r\n",
        "#Añadimos capas densas \r\n",
        "model.add(tf.keras.layers.Dense(64, activation='selu'))\r\n",
        "#Al tener 3 clasificaciones(pos, neg y neutro) se introduce una capa de salida con 3 neuronas \r\n",
        "model.add(tf.keras.layers.Dense(3, activation='softmax'))\r\n",
        "#Compilamos el modelo\r\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc']) "
      ],
      "outputs": [],
      "metadata": {
        "id": "EowqoqHcOfU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Entrenaremos nuestro modelo de atención por épocas de 5 en mini lotes de muestras de 100.\r\n",
        "BATCH_SIZE = 100\r\n",
        "EPOCHS = 30\r\n",
        "history = model.fit(x_train,y_train,\r\n",
        "                    batch_size=BATCH_SIZE,\r\n",
        "                    epochs=EPOCHS,\r\n",
        "                    validation_split=0.2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "RIPzC_3mOfU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Evaluamos sobre test \r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "prediction = model.predict(x_test)\r\n",
        "y_pred = (prediction > 0.5)\r\n",
        "accuracy_score(y_test, np.argmax(y_pred, axis=1))"
      ],
      "outputs": [],
      "metadata": {
        "id": "EzcOflsAOfU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Definimos la matriz de confusion y la ploteamos\r\n",
        "import numpy as np\r\n",
        "import matplotlib as mpl\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.metrics import (classification_report,\r\n",
        "                             confusion_matrix,\r\n",
        "                             roc_auc_score)\r\n",
        "%matplotlib inline\r\n",
        "%config InlineBackend.figure_format = 'retina'\r\n",
        "\r\n",
        "\r\n",
        "def plot_cm(labels, predictions, p=0.5):\r\n",
        "    cm = confusion_matrix(y_test, np.argmax(y_pred,axis=1))\r\n",
        "    plt.figure(figsize=(5, 5))\r\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\")\r\n",
        "    plt.title(\"Confusion matrix (non-normalized))\")\r\n",
        "    plt.ylabel(\"Actual label\")\r\n",
        "    plt.xlabel(\"Predicted label\")\r\n",
        "\r\n",
        "\r\n",
        "plot_cm(y_test, y_pred)"
      ],
      "outputs": [],
      "metadata": {
        "id": "rPdPygbYOfU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "Lbmi22xXOfU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install -q tensorflow-text\r\n",
        "!pip install -q tf-models-official\r\n",
        "!pip install -q -U keras-tuner\r\n",
        "import kerastuner as kt\r\n",
        "\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "import tensorflow_text as text\r\n",
        "import official.nlp.optimization\r\n",
        "from official.nlp import optimization  # to create AdamW optmizer\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "outputs": [],
      "metadata": {
        "id": "UGC_LjiMT5Ti"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#shuffleamos el dataset, en este ejercicio usaremos el texto crudo\r\n",
        "from sklearn.utils import shuffle\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataclean['text'],\r\n",
        "                                                    dataclean['sentiment'],\r\n",
        "                                                    test_size=0.2)\r\n",
        "data = pd.DataFrame()\r\n",
        "data['text'] = pd.Series(X_train)\r\n",
        "data['sentiment'] = pd.Series(y_train)\r\n",
        "\r\n",
        "datatest = pd.DataFrame()\r\n",
        "datatest['text'] = pd.Series(X_train)\r\n",
        "datatest['sentiment'] = pd.Series(y_train)\r\n",
        "\r\n",
        "dataset = shuffle(data).reset_index(\r\n",
        "  drop=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ovu_JwvtOfU4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dataset"
      ],
      "outputs": [],
      "metadata": {
        "id": "Qugy9JqROfU4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Aqui encontramos los diferentes modelos de bert y al final de la celda seleccionamos el que vamos a usar\r\n",
        "\r\n",
        "bert_model_name = 'small_bert/bert_en_uncased_L-2_H-128_A-2'\r\n",
        "\r\n",
        "map_name_to_handle = {\r\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\r\n",
        "    'bert_en_cased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\r\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\r\n",
        "    'albert_en_base':\r\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\r\n",
        "    'electra_small':\r\n",
        "        'https://tfhub.dev/google/electra_small/2',\r\n",
        "    'electra_base':\r\n",
        "        'https://tfhub.dev/google/electra_base/2',\r\n",
        "    'experts_pubmed':\r\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\r\n",
        "    'experts_wiki_books':\r\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\r\n",
        "    'talking-heads_base':\r\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\r\n",
        "}\r\n",
        "\r\n",
        "map_model_to_preprocess = {\r\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'bert_en_cased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/1',\r\n",
        "    'albert_en_base':\r\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/1',\r\n",
        "    'electra_small':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'electra_base':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'experts_pubmed':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'experts_wiki_books':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "    'talking-heads_base':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\r\n",
        "}\r\n",
        "\r\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\r\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]"
      ],
      "outputs": [],
      "metadata": {
        "id": "o19_7rFZOfU4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Construimos el modelo clasificador\r\n",
        "def build_classifier_model(encoder, processer, epochs_set, initial_learning_rate):\r\n",
        "    # Definición de la entrada del modelo\r\n",
        "    text_input = tf.keras.layers.Input(\r\n",
        "        shape=(),\r\n",
        "        dtype=tf.string,\r\n",
        "        name='text'\r\n",
        "    )\r\n",
        "    # Definición del preprocesador para bert\r\n",
        "    preprocessing_layer = hub.KerasLayer(\r\n",
        "        processer,\r\n",
        "        name='preprocessing'\r\n",
        "    )\r\n",
        "    encoder_inputs = preprocessing_layer(text_input)\r\n",
        "\r\n",
        "    # Definición del Modelo Bert para codificar la información.\r\n",
        "    encoder = hub.KerasLayer(\r\n",
        "        encoder,\r\n",
        "        trainable=True,\r\n",
        "        name='BERT_encoder'\r\n",
        "    )\r\n",
        "    outputs = encoder(encoder_inputs)\r\n",
        "   \r\n",
        "    # Seleccionamos la representación del documento entero.\r\n",
        "    net = outputs['pooled_output']\r\n",
        "   \r\n",
        "    # Definimos la arquitectura del modelo de salida ajustada a la tarea\r\n",
        "    net = tf.keras.layers.Dropout(0.5)(net)\r\n",
        "    net = tf.keras.layers.Dense(units = 1024, activation='selu')(net)\r\n",
        "    net = tf.keras.layers.Dense(units = 512, activation='selu')(net)\r\n",
        "    net = tf.keras.layers.Dense(units = 256, activation='selu')(net)\r\n",
        "    net = tf.keras.layers.Dense(units = 128, activation='selu')(net)\r\n",
        "    net = tf.keras.layers.Dense(units = 64, activation='selu')(net)\r\n",
        "    net = tf.keras.layers.Dense(units = 32, activation='selu')(net)\r\n",
        "    net = tf.keras.layers.Dense(units = 16, activation='selu')(net)\r\n",
        "    net = tf.keras.layers.Dense(8, activation='selu')(net)  \r\n",
        "    net = tf.keras.layers.Dense(3, activation='softmax', name='classifier')(net)\r\n",
        "    \r\n",
        "    classifier_model = tf.keras.Model(text_input, net)\r\n",
        "    \r\n",
        "    # Definición del optimizador para el problema\r\n",
        "    epochs = epochs_set\r\n",
        "    steps_per_epoch = dataset.shape[0] # 625\r\n",
        "    num_train_steps = steps_per_epoch * epochs\r\n",
        "    num_warmup_steps = int(0.1*num_train_steps)\r\n",
        "   \r\n",
        "    optimizer = optimization.create_optimizer(\r\n",
        "        init_lr=initial_learning_rate,\r\n",
        "        num_train_steps=num_train_steps,\r\n",
        "        num_warmup_steps=num_warmup_steps,\r\n",
        "        optimizer_type='adamw'\r\n",
        "      )\r\n",
        "    # Compilación del modelo\r\n",
        "    classifier_model.compile(\r\n",
        "        optimizer=optimizer,\r\n",
        "        loss=\"sparse_categorical_crossentropy\",\r\n",
        "        metrics=[\"accuracy\"]\r\n",
        "      )\r\n",
        "\r\n",
        "    \r\n",
        "    return classifier_model \r\n",
        "\r\n",
        "    "
      ],
      "outputs": [],
      "metadata": {
        "id": "xkDifsxQOfU6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Con la funcion que hemos creado en la ultima celda definimos el modelo y mostramos el resumen en pantalla\r\n",
        "epocas = 200\r\n",
        "\r\n",
        "model = build_classifier_model(\r\n",
        "    tfhub_handle_encoder,\r\n",
        "    tfhub_handle_preprocess,\r\n",
        "    epocas,\r\n",
        "    5e-5\r\n",
        ")\r\n",
        "model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "id": "PfpwVq4XOfU7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Entrenamos el modelo\r\n",
        "history = model.fit(x=dataset.text,\r\n",
        "                    y=dataset.sentiment,\r\n",
        "                    validation_split=0.2,\r\n",
        "                    epochs=220,\r\n",
        "                    )"
      ],
      "outputs": [],
      "metadata": {
        "id": "cBQCwI5jRf5O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Para comprobar resultados procedemos a hacer una particion en 10 partes y entrenamos con las distintas particiones y evaluamos resultados.\r\n",
        "\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "n_split = 10\r\n",
        "\r\n",
        "epocas = 30\r\n",
        "eval = []\r\n",
        "numero = 1\r\n",
        "for train_index,test_index in KFold(n_split).split(dataset):\r\n",
        "  x_train,x_test=dataset.text[train_index],dataset.text[test_index]\r\n",
        "  y_train,y_test=dataset.sentiment[train_index],dataset.sentiment[test_index]\r\n",
        "  \r\n",
        "  model = build_classifier_model(\r\n",
        "    tfhub_handle_encoder,\r\n",
        "    tfhub_handle_preprocess,\r\n",
        "    epocas,\r\n",
        "    3e-5\r\n",
        "  )\r\n",
        "\r\n",
        "  model.fit(x_train, y_train,epochs=epocas)\r\n",
        "  eval.append(model.evaluate(x_test,y_test))\r\n",
        "  print('Model evaluation ',model.evaluate(x_test,y_test))\r\n",
        "  numero=numero+1"
      ],
      "outputs": [],
      "metadata": {
        "id": "4UMbu4ZaRtCY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Evaluamos sobre test para realizar la misma evaluacion que en los modelos anteriores y que la comparacion sea justa\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "prediction = model.predict(X_test)\r\n",
        "y_pred = (prediction > 0.5)\r\n",
        "accuracy_score(y_test, np.argmax(y_pred, axis=1))"
      ],
      "outputs": [],
      "metadata": {
        "id": "hhmScbRhVn6Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#Definimos la matriz de confusion y la ploteamos\r\n",
        "import numpy as np\r\n",
        "import matplotlib as mpl\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.metrics import (classification_report,\r\n",
        "                             confusion_matrix,\r\n",
        "                             roc_auc_score)\r\n",
        "%matplotlib inline\r\n",
        "%config InlineBackend.figure_format = 'retina'\r\n",
        "\r\n",
        "\r\n",
        "def plot_cm(labels, predictions, p=0.5):\r\n",
        "    cm = confusion_matrix(y_test, np.argmax(y_pred,axis=1))\r\n",
        "    plt.figure(figsize=(5, 5))\r\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\")\r\n",
        "    plt.title(\"Confusion matrix (non-normalized))\")\r\n",
        "    plt.ylabel(\"Actual label\")\r\n",
        "    plt.xlabel(\"Predicted label\")\r\n",
        "\r\n",
        "\r\n",
        "plot_cm(y_test, y_pred)"
      ],
      "outputs": [],
      "metadata": {
        "id": "1GK2RkphWFGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Respecto al rendimiento de los modelos, consideranco que el benchmark tiene un 0.98 de accuracy estqamos un poco lejos en la mayoria, aunque son rendimientos aceptables. Vemos como el mejor accuracy lo proporciona el modelo bert, aunque los otros modelos realizados obtienen resultados decentes. Personalmente me sorprende los buenos resultados obtenidos por el modelo que combina capas recurrentes y convolucionales ya que no esperaba que un modelo a priori tan sencillo obtuviera esos resultados, pero predice muy mal una de las clases(negativa).\n",
        "Tambien observamos que todos los modelos terminan sobreajustando en mayor o menor medida a pesar de incluir capas de regularizacion. "
      ],
      "metadata": {
        "id": "ZkOYKlvMWPF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizamos un modelo de sentimiento ya implementado para comparar con el nuestro posteriormente"
      ],
      "metadata": {
        "id": "Z-c_fZ0lWYpq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\r\n",
        "nltk.download('vader_lexicon')\r\n",
        "sia = SentimentIntensityAnalyzer()\r\n",
        "result=[]\r\n",
        "for i in range(0,len(X_test)):\r\n",
        "  b = sia.polarity_scores(X_test.iloc[i])\r\n",
        "  del b['compound']\r\n",
        "  result.append(b)"
      ],
      "outputs": [],
      "metadata": {
        "id": "7F7D2MfxWJiw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "result =pd.DataFrame.from_dict(result)\r\n",
        "result.head()\r\n",
        "\r\n",
        "result.columns =[0, 1,2] "
      ],
      "outputs": [],
      "metadata": {
        "id": "H2NX6KufWdKo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pred = result.idxmax(axis=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "3-jeFXbwWhIJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "accuracy_score(y_test, pred)"
      ],
      "outputs": [],
      "metadata": {
        "id": "W2hUfqgPWjsL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plot_cm(y_test, pred)"
      ],
      "outputs": [],
      "metadata": {
        "id": "IYY60R_XWl3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observamos que el accuracy para un modelo ya implementado es menor que el que hemos conseguido en todos los modelos, por lo que se podria decir que los modelos conseguidos son mejores que el modelo ya implementado."
      ],
      "metadata": {
        "id": "UzTxkoozWtqE"
      }
    }
  ]
}